---
layout: page
title: Happy Robotics 2018
date: 
type: page
parent_id: '0'
published: false
password: ''
status: private
categories: []
tags: []
meta:
  _edit_last: '2'
  _wp_page_template: default
  views: '16'
author:
  login: demu
  email: info@demura.net
  display_name: demu
  first_name: ''
  last_name: ''
permalink: "/happyrobotics"
---
<p><strong>The Happy Robotics team official web site</strong></p>
<p><span style="color: #ff0000;">Happy mini, our human support robot, is the 3rd place in sympathy and 5th place in design rating in the @Home league in RoboCup2016 Leipzig by the research of Eva Jahn as shown in <a href="https://www.researchgate.net/publication/312948944_RoboCup_WM_2016_Leipzig_-_home_liga_-_design_and_sympathy_rating">this web site</a>.  Many wonderful robots, such as Pepper, HSR, R1, SocRob, Amigo, Tiago and so on, participated in RoboCup@Home2016.</span></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=FUKtgivErqE">Qualification Video</a></li>
</ul>
<p><iframe src="https://www.youtube.com/embed/FUKtgivErqE" width="1028" height="511" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<ul>
<li><strong>Vision</strong></li>
</ul>
<p>The Happy Robotics team has been participating in the RoboCup@Home league of the RoboCup Japan Open since 2012, and participated in the RoboCup 2015, 2016, and 2017 world competition. The team got the 9th, 8th, and 9th places in the competition, respectively. It is our first attempt to participate in World Robot Summit 2018.</p>
<p>The vision of the Happy Robotics team is “Making the World Happy by Making a Kawaii Robot.” “Kawaii'' means cute, lovely, or charming. We think that the kawaii robot can solve problems of an aging society in Japan, life support robots should be more kawaii. Happy Robot as shown in Fig.1 is designed in the image of a little child with a lively yellow color. Happy Robot is considered to be the first kawaii designed robot in the RoboCup@Home and might be in World Robot Summit. The software system was based on ROS, and the state of arts frame works such as Caffe, Darknet, Tensorflow, Digits, Hark, Kaldi and so on.</p>
<ul>
<li><strong>Robot</strong></li>
</ul>
<p style="text-align: justify;">(1) Hardware</p>
<p style="text-align: justify;">We have been developing 2 kinds of human support robots.  One is a childcare robot for children and another is a wheelchair robot for the elderly. To foster the research and development, the Demura Research Laboratory has been participating in RoboCup@Home since 2012 and Tsukuba Challenge since 2008.</p>
<p>The design concepts of  Happy Mini are Kawaii (lovely, cute), simplicity, safety,  and usability.    <span style="color: #ff00ff;">Kawaii</span> is the most important aspect of human-robot interaction for Mini. Mini is an infant robot, and the ability is limited as well as other robots in RoboCup. So, Mini needs the help of human, that is why <span style="color: #ff00ff;">kawaii</span> of appearance and voice is crucial for Mini. The exterior of Mini is roundish and lively yellow as shown in Fig.1. The technical information of these robots is described in our team description paper(HappyRoboticsTDP2018).</p>
<p><a href="http://demura.net/happymini2017/attachment/cad2" rel="attachment wp-att-13240"><img src="{{ site.baseurl }}/assets/images/cad2-246x300.png" alt="" width="246" height="300" class="size-medium wp-image-13240 alignleft" /></a><img src="{{ site.baseurl }}/assets/images/happyMiniNewArm2-156x300.jpg" alt="" width="156" height="300" class="size-medium wp-image-13239 alignleft" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p style="text-align: center;">Happy mini (Left: CAD original design,  Right: Final real robot)</p>
<p><a href="http://demura.net/kithappyrobot2018/attachment/kawaii" rel="attachment wp-att-13854"><img src="{{ site.baseurl }}/assets/images/kawaii.png" alt="" width="547" height="410" class="aligncenter size-full wp-image-13854" /></a></p>
<p style="text-align: center;">Fig.1  Happy Robot Family</p>
<p><a href="http://demura.net/robocupathome/attachment/torso" rel="attachment wp-att-12605"><img src="{{ site.baseurl }}/assets/images/torso.jpg" alt="torso" width="470" height="321" class="aligncenter size-full wp-image-12605" /></a></p>
<p style="text-align: center;">Fig.2   Extendable  Torso</p>
<p style="text-align: center;">Table.2  Specification</p>
<table>
<tbody>
<tr>
<td width="153"></td>
<td width="154">Happy Mini</td>
</tr>
<tr>
<td width="153">Height [m]</td>
<td width="154">1.10~1.50</td>
</tr>
<tr>
<td width="153">Width [m]</td>
<td width="154">0.32</td>
</tr>
<tr>
<td width="153">Length [m]</td>
<td width="154">0.32</td>
</tr>
<tr>
<td width="153">Weight [kg]</td>
<td width="154">10.0</td>
</tr>
<tr>
<td width="153">Max speed [m/s]</td>
<td width="154">0.7</td>
</tr>
<tr>
<td width="153">D.O.F</td>
<td width="154">5</td>
</tr>
</tbody>
</table>
<p>(2) Software</p>
<p><strong>Object Recognition &amp; Manipulation</strong>: We develop a cascade deep neural network of YOLO (You Only Look Once) and a conventional CNN, such as LeNet, and AlexNet. Firstly, we use a pretrained YOLO for object detection and rough classification. Secondly, the CNN classifies object more precisely using outputs of YOLO.  The merit of this cascade deep neural network is very fast learning. Because we just use a pretrained YOLO, only learn the data set of RoboCup@Home competition using LeNet or AlexNet. The result of object recognition is shown in Fig.3. Next, the robot removes the planer and outlier from point cloud data, and calculates the centroid of clustering from point cloud corresponded object image area. This calculation uses Point Cloud Library. Finally, the robot calculates Inverse Kinematics from object center of gravity and grasp it.</p>
<p><a href="http://demura.net/kithappyrobot2018/attachment/objects" rel="attachment wp-att-13856"><img src="{{ site.baseurl }}/assets/images/objects.png" alt="" width="640" height="480" class="aligncenter size-full wp-image-13856" /></a></p>
<p style="text-align: center;">Fig. 3  Results of object recognition</p>
<p><strong>Perception:</strong> The software system is based on ROS and Caffe. Several algorithms are used for an object and human face recognition. HaarCascade classifier, Flat Clustering filtering, and Deep Neural Networks (DNNs) are used for face recognition (Fig.4). Nesterov’s accelerated gradient and Convolution DNNs are used for gender classification. We trained the network using 700 samples for each category, and accuracy of classification is about 75 %.</p>
<p>The real-time object detection deep neural newtork, <a href="https://pjreddie.com/darknet/yolo/">YOLO (You only look once)</a> is used for person detection as shown in Fig.5. The algorithm is very fast compared to other DNNs and robust.</p>
<p><a href="http://demura.net/robocupathome/attachment/face" rel="attachment wp-att-12616"><img src="{{ site.baseurl }}/assets/images/face.jpg" alt="face" width="603" height="167" class="size-full wp-image-12616" /></a></p>
<p style="text-align: center;">Fig.4   Face recognition using HarrCascade and DNN</p>
<p style="text-align: left;"><a href="http://demura.net/happymini2017/attachment/yolo" rel="attachment wp-att-13238"><img src="{{ site.baseurl }}/assets/images/yolo-1024x662.png" alt="" width="800" height="517" class="aligncenter size-large wp-image-13238" /></a></p>
<p style="text-align: center;">Fig.5  Person detection</p>
<p>S<strong>peech Recognition and Sound Localization: </strong>We have developed a speech recognition and a sound source localization system. The speech recognition system uses the <a href="http://kaldi-asr.org/">Kaldi </a>based <a href="https://github.com/alumae/kaldi-gstreamer-server">gstream server</a>. It is a real-time full-duplex speech recognition server, and<br />
uses a DNN-based model for English trained on the TEDLIUM speech corpus.</p>
<p>The sound source localization system is implemented using the robot auditory library <a href="http://www.hark.jp/">HARK</a>. The HARK easily be programmed by GUI as shown in Fig.6. The system takes multi-channel speech waveform data from the 8ch microphone array, calculates FFT and estimates the sound source direction by the MUSIC method.</p>
<p><a href="http://demura.net/happymini2017/attachment/hark" rel="attachment wp-att-13247"><img src="{{ site.baseurl }}/assets/images/hark-1024x576.png" alt="" width="800" height="450" class="aligncenter size-large wp-image-13247" /></a></p>
<p style="text-align: center;">Fig.6  HARK</p>
<p style="text-align: left;"><strong>Simulator:</strong> We have developed the Happy Robot simulator using the Gazebo robot simulator as shown in Fig.7. To make the 3D Happy Robot model in the simulator, we described the URDF (Universal Robot-ics Description Format) file and imported 3D meshes from the 3D CAD data. MoveIt! is easily applied to the 3D model described by the URDF</p>
<p style="text-align: center;"><a href="http://demura.net/robocupathome/attachment/simulator" rel="attachment wp-att-12617"><img src="{{ site.baseurl }}/assets/images/simulator.jpg" alt="simulator" width="618" height="177" class="aligncenter size-full wp-image-12617" /></a></p>
<p style="text-align: center;">Fig.7  The Happy Mini Simulator using Gazebo</p>
<ul>
<li><strong>Scientific achievements</strong></li>
</ul>
<p><strong>Developing an autonomous personal mobility:　</strong>The first author’s laboratory has been developing an autonomous personal mobility called UNiMO AI as shown in Fig. 8 based on the commercially available crawler typed electric wheel chair. The conventional wheelchair is difficult to get over small difference (about 30 [mm]) in level, and it sometimes makes a falling accident. To prevent the accident, the capability of detecting the small difference is required. UNiMO AI can detect the small difference and has the ability to cross steps of 10cm.</p>
<p><a href="http://demura.net/kithappyrobot2018/attachment/unimo-2" rel="attachment wp-att-13832"><img src="{{ site.baseurl }}/assets/images/unimo.jpg" alt="" width="960" height="540" class="aligncenter size-full wp-image-13832" /></a></p>
<p style="text-align: center;">Fig.8  An autonomous personal mobility robot: UNiMO AI</p>
<p style="text-align: center;">Table 2  Specification</p>
<table>
<tbody>
<tr>
<td width="153"></td>
<td width="154">UNiMO AI</td>
</tr>
<tr>
<td width="153">Width x Length x Height [m]</td>
<td width="154">1.00 x 0.695 x 0.88</td>
</tr>
<tr>
<td width="153">Weight [kg]</td>
<td width="154">98.5</td>
</tr>
<tr>
<td width="153">Max speed [m/s]</td>
<td width="154">1.1</td>
</tr>
<tr>
<td width="153">Drive mechanism</td>
<td width="154">Crawler,  400W AC Motor x 2</td>
</tr>
<tr>
<td width="153">Computer</td>
<td width="154">NVIDIA Jetson TX2  x 2</td>
</tr>
<tr>
<td width="153">Sensors</td>
<td width="154">2D Lidar, 3D Lidar, IMU, FOG, Camera,</td>
</tr>
</tbody>
</table>
<p><strong>Cascade deep neural network of Yolo and CNNs: </strong>Detection of a specific person is not easy especially in the outdoor environment. We have developing the detection system that is the cascade deep neural network of Yolo and AlexNet. Firstly, Yolo is one of the state arts of deep neural network, and it can segment regions and classify objects. However, the classification ability is not so good. Thus, we propose a cascade of Yolo and other CNNs such as AlexNet, Goog-LeNet. Fig. 9 shows the results of Yolo and AlexNet. The system is very robust even in the outdoor environment.</p>
<p><a href="http://demura.net/kithappyrobot2018/attachment/target-jpg" rel="attachment wp-att-13839"><img src="{{ site.baseurl }}/assets/images/target.jpg.png" alt="" width="710" height="527" class="aligncenter size-full wp-image-13839" /></a></p>
<p style="text-align: center;">Fig. 9  Detection of a specific person in the outdoor environment</p>
<p><strong>Road unevenness detection:</strong> We have been developing a wheelchair robot and a driver assist technology system for the elderly. The conventional wheelchair is difficult to get over small difference (about 30 [mm]) in level, and it sometimes makes a falling accident. To prevent the accident, the capability of detecting the small difference is required.  We have developed two system to solve the problem.</p>
<p>The first system uses the Kinect V2 sensor as shown in Fig.10, and the second system uses the 3D Rolling Lidar developed by demura.net as shown in Fig.11. As far as our knowledge,  it is the most advanced technology to detect small difference in level for a wheelchair size robot.    Developing the small difference level detector for a commercial wheelchair and a mobility scooter for elderly, and applying the technology to a human support robot is our near future work.</p>
<p><a href="http://demura.net/robocupathome/attachment/kinect_method" rel="attachment wp-att-12606"><img src="{{ site.baseurl }}/assets/images/kinect_method.jpg" alt="kinect_method" width="616" height="197" class="aligncenter size-full wp-image-12606" /> </a>                Fig.10  The results of detection of 30 [mm] difference in level by the Kinect V2.</p>
<p><a href="http://demura.net/robocupathome/attachment/rollinglidar" rel="attachment wp-att-12607"><img src="{{ site.baseurl }}/assets/images/rollingLidar.jpg" alt="rollingLidar" width="661" height="224" class="aligncenter size-full wp-image-12607" /></a></p>
<p style="text-align: center;">Fig.11 The results of detection of 25 [mm] difference in level by the Rolling 3D Lidar</p>
<p>&nbsp;</p>
<ul>
<li> Team Description Paper
<ul>
<li>HappyRoboticsTDP2018</li>
</ul>
</li>
<li>Software
<ul>
<li><a href="https://github.com/demulab">Current out GitHub</a></li>
<li>RoboCup2015 @Home League in Heifei, China (9th place)
<ul>
<li><a href="https://github.com/demulab">GitHub for DemuLab (demura.net)</a></li>
</ul>
</li>
<li>RoboCup Japan Open 2014 @Home Simulation League  (3rd place)
<ul>
<li><a href="http://demura.net/wordpress/wp-content/uploads/2015/01/JapanOpen2014.tar.gz">SourceCode.tgz</a> by Kensei and Kosei Demura</li>
</ul>
</li>
<li>RoboCup@Home 2014: What did you say ?
<ul>
<li><a href="http://demura.net/robocup/11705.html">How to make a SIRI like app ?</a></li>
</ul>
</li>
<li><a href="http://www.amazon.co.jp/gp/product/4627846916?ie=UTF8&amp;tag=demuranet-22&amp;linkCode=as2&amp;camp=247&amp;creative=1211&amp;creativeASIN=4627846916">The Book: Robot Simulation with Open Dynamics Engine (by Kosei Demura)</a>
<ul>
<li><a href="http://demura.net/wordpress/wp-content/uploads/2015/01/roboSimu120409.zip">Sample programs in the book</a></li>
<li>PowerPoint for lectures
<ul>
<li><a href="http://demura.net/wordpress/wp-content/uploads/2015/01/RoboSimuODE090109.ppt">ODE</a>,  <a href="http://demura.net/wordpress/wp-content/uploads/2015/01/RoboSimuWheeled080627.ppt">Wheeled Robot</a>, <a href="http://demura.net/wordpress/wp-content/uploads/2015/01/RoboSimuArm080627.ppt">Robot Arm</a>, <a href="http://demura.net/wordpress/wp-content/uploads/2015/01/RoboSimu4Legged080627.ppt">Legged Robot</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Team
<ul>
<li>Adviser: Kosei Demura</li>
<li>Member: Koyo Enomoto (Leader),  Kazuki Nagashima, Shogo Okano, Haruya Yuda</li>
</ul>
</li>
<li>Previous participation in RoboCup@Home
<ul>
<li>RoboCup Japan Open 2012
<ul>
<li>@Home Real Robot League: Lost in a preliminary round</li>
</ul>
</li>
<li>RoboCup Japan Open 2013
<ul>
<li>@Home Real Robot League: Lost in a preliminary round</li>
<li>@Home Simulation League:</li>
</ul>
</li>
<li>RoboCup Japan Open 2014
<ul>
<li>@Home Real Robot League: 8th place</li>
<li>@Home  Simulation League: 3rd place (Team name: Yumekobo Junior)</li>
</ul>
</li>
<li>RoboCup Japan Open 2015
<ul>
<li>@Home Real Robot League: 5th place</li>
<li>@Home  Simulation League: 5th place</li>
</ul>
</li>
<li>RoboCup 2015, Heifei,  China
<ul>
<li>@Home: 9th place</li>
</ul>
</li>
<li>RoboCup Japan Open 2016
<ul>
<li>@Home Education: 1st place</li>
<li>RoboCup 2016, Leipzig, Germany<br />
@Home: 8th place</li>
</ul>
</li>
<li>RoboCup Japan Open 2017
<ul>
<li>@Home Open Platform League: 4th place</li>
</ul>
</li>
<li>RoboCup 2017, Nagoya, Japan
<ul>
<li>@Home Open Platform: 9th place</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
